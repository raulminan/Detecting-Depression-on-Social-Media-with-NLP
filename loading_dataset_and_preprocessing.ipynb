{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loading the dataset\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for text processing\n",
    "import nltk\n",
    "\n",
    "#for saving the preprocessed dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our starting point is a collection of XML files collected in [this paper by David E. Losada et al.](https://tec.citius.usc.es/ir/pdf/CLEF16_paper.pdf). Each XML stores a sequence of submissions, including date, text and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml2dataframe(file_path):\n",
    "  \"\"\"\n",
    "  Takes a xml (one per subject) file and returns a \n",
    "  dataframe with 1 column:\n",
    "  |-------------------------- |\n",
    "  | TEXT                      |\n",
    "  |---------------------------|\n",
    "  |\"All posts from subject 0\" |\n",
    "  |---------------------------|\n",
    "  |\"All posts from subject 1\" |\n",
    "  |---------------------------|\n",
    "  ....\n",
    "\n",
    "  \"\"\"\n",
    "  tree = ET.parse(file_path)\n",
    "  root = tree.getroot()\n",
    "  all_posts = []\n",
    "\n",
    "  for i in root.iter(\"WRITING\"): #Iterate over the \"WRITING\" elements\n",
    "    post = i.find(\"TEXT\").text #Get the \"TEXT\" parts (these contain the posts)\n",
    "    all_posts.append(post) #get a list of all posts, each one is a string\n",
    "\n",
    "  one_post = \" \".join(all_posts) #join all posts into a single string\n",
    "  one_post_list = [] #we need the one_post string to be a list\n",
    "  one_post_list.append(one_post)\n",
    "\n",
    "  #create a dataframe with all the posts (one post per row)\n",
    "  df = pd.DataFrame(one_post_list, columns=[\"TEXT\"]) \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_df():\n",
    "\n",
    "\t\"\"\"loads the positive and negative subjects from the \n",
    "\ttest data\"\"\"\n",
    "\n",
    "\ttest_neg_folder = \"filepath\"\n",
    "\ttest_pos_folder = \"filepath\"\n",
    "\n",
    "\ttest_positive_dfs, test_negative_dfs = [], []\n",
    "\n",
    "\n",
    "\tfor file in os.listdir(test_pos_folder): \n",
    "\t\tfilename = test_pos_folder + \"/\" + os.fsdecode(file)\n",
    "\t\ttest_pos_df = xml2dataframe(filename)\n",
    "\t\ttest_positive_dfs.append(test_pos_df)\n",
    "\n",
    "\ttest_pos_big_df = pd.concat(test_positive_dfs)\n",
    "\ttest_pos_big_df[\"LABEL\"] = 1 #add 1 for \"depressed\" subjects in the positive folder\n",
    "\n",
    "\tfor file in os.listdir(test_neg_folder): \n",
    "\t\tfilename = test_neg_folder + \"/\" + os.fsdecode(file)\n",
    "\t\ttest_neg_df = xml2dataframe(filename)\n",
    "\t\ttest_negative_dfs.append(test_neg_df)\n",
    "\n",
    "\ttest_neg_big_df = pd.concat(test_negative_dfs)\n",
    "\ttest_neg_big_df[\"LABEL\"] = 0 #add 0 for \"non-depressed\" subjects in the negative folder\n",
    "\n",
    "\ttest_df = test_pos_big_df.append(test_neg_big_df)\n",
    "\treturn test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_df():\n",
    "\t\"\"\"loads the positive and negative subjects from the \n",
    "\ttraining data\"\"\"\n",
    "\t\n",
    "\ttrain_neg_folder = \"filepath\"\n",
    "\ttrain_pos_folder = \"filepath\"\n",
    "\n",
    "\ttrain_positive_dfs, train_negative_dfs = [], []\n",
    "\n",
    "\n",
    "\tfor file in os.listdir(train_pos_folder): \n",
    "\t\tfilename = train_pos_folder + \"/\" + os.fsdecode(file)\n",
    "\t\ttrain_pos_df = xml2dataframe(filename)\n",
    "\t\ttrain_positive_dfs.append(train_pos_df)\n",
    "\n",
    "\ttrain_pos_big_df = pd.concat(train_positive_dfs)\n",
    "\ttrain_pos_big_df[\"LABEL\"] = 1 \n",
    "\n",
    "\tfor file in os.listdir(train_neg_folder): \n",
    "\t\tfilename = train_neg_folder + \"/\" + os.fsdecode(file)\n",
    "\t\ttrain_neg_df = xml2dataframe(filename)\n",
    "\t\ttrain_negative_dfs.append(train_neg_df)\n",
    "\n",
    "\ttrain_neg_big_df = pd.concat(train_negative_dfs)\n",
    "\ttrain_neg_big_df[\"LABEL\"] = 0\n",
    "\n",
    "\ttraining_df = train_pos_big_df.append(train_neg_big_df)\n",
    "\treturn training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = create_test_df()\n",
    "training_df = create_training_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check composition of the dataset\n",
    "df_comp = pd.concat([training_df, test_df]) \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Class distribution\", fontsize=12)\n",
    "df_comp[\"LABEL\"].reset_index().groupby(\"LABEL\").count().sort_values(by= \n",
    "       \"index\").plot(kind=\"barh\", legend=False, \n",
    "        ax=ax).grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is very umbalanced, which might create some problems in our model later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "  def __init__(self):\n",
    "    nltk_resources = [\"punkt\", \"averaged_perceptron_tagger\", \"wordnet\", \"stopwords\", \"words\"]\n",
    "    #check if resource is downloaded, if not, download it\n",
    "    for resource in nltk_resources:\n",
    "      try:\n",
    "        nltk.data.find(resource)\n",
    "      except LookupError:\n",
    "        nltk.download(resource)\n",
    "\n",
    "    self.lemmatizer = nltk.WordNetLemmatizer()\n",
    "    self.words = set(nltk.corpus.words.words())\n",
    "    self.stopwords = set(nltk.corpus.stopwords.words())\n",
    "\n",
    "  def nltk_tag_to_wordnet_tag(self, nltk_tag):\n",
    "    \"\"\"Changes the nltk pos tags to wordnet tags to be tagged correctly\n",
    "    in POS tagging\n",
    "    \"\"\"\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "  def lemmatize_text(self, text):\n",
    "    \"\"\"Lemmatizes the POS_tagged input text\"\"\"\n",
    "\n",
    "    wordnet_tagged = map(lambda x: (x[0], self.nltk_tag_to_wordnet_tag(x[1])), text) #changes the tags to wordnet\n",
    "    lemmatized = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "          #if there is no available tag, append the token as is\n",
    "          lemmatized.append(word)\n",
    "        #expand common contractions\n",
    "        elif word in [\"'m\", \"'s\", \"'re\", \" 'm\", \" 's\", \" 're\"]: \n",
    "          lemmatized.append(\"be\")\n",
    "        elif word == \"'ve\":\n",
    "          lemmatized.append(\"have\")\n",
    "        elif word == \"n't\":\n",
    "          lemmatized.append(\"not\")\n",
    "        else:\n",
    "          #else use the tag to lemmatize the token\n",
    "          lemmatized.append(self.lemmatizer.lemmatize(word, tag))\n",
    "\n",
    "    return lemmatized\n",
    "\n",
    "  def preprocessing(self, dataframe):\n",
    "    \"\"\"\n",
    "    Takes a dataframe as input and preprocesses the data as follows:\n",
    "      1. Tokenization\n",
    "      2. Part of Speech Tagging\n",
    "      3. Lemmatization\n",
    "      4. Removes punctuation\n",
    "      5. Removes non-words and lowecases all words\n",
    "      6. Convert to string\n",
    "    \"\"\"\n",
    "    # 1. Tokenization\n",
    "    dataframe[\"TOKENIZED_TEXT\"] = dataframe.apply(lambda row: nltk.word_tokenize(row['TEXT']), axis=1)\n",
    "\n",
    "    # 2. POS Tagging\n",
    "    dataframe[\"POS_TAGGED_TEXT\"] = nltk.pos_tag_sents(dataframe[\"TOKENIZED_TEXT\"])\n",
    "\n",
    "    # 3. Lemmatization\n",
    "    dataframe['LEMMATIZED_TEXT'] = dataframe['POS_TAGGED_TEXT'].apply(lambda row: self.lemmatize_text(row))\n",
    "\n",
    "    #4. Remove punctuation\n",
    "    dataframe[\"LEMMATIZED_TEXT\"] = dataframe.apply(\n",
    "        lambda row: [word for word in row[\"LEMMATIZED_TEXT\"] if word.isalpha()], axis=1)\n",
    "\n",
    "    #5. Non-word and stop word removal + lowercasing\n",
    "    dataframe[\"FINAL_TEXT\"] = dataframe.apply(\n",
    "        lambda row: [w.lower() for w in row[\"LEMMATIZED_TEXT\"] if w in self.words and w not in self.stopwords], \n",
    "        axis=1)\n",
    "\n",
    "    #6. Convert to string\n",
    "    dataframe['FINAL_TEXT_STRING'] = [' '.join(map(str, l)) for l in dataframe['FINAL_TEXT']]\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\raulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\raulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\raulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\raulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\raulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\raulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\raulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\raulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\raulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\raulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "final_training_df = TextPreprocessor().preprocessing(training_df)\n",
    "final_test_df = TextPreprocessor().preprocessing(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save datasets as pickle objects\n",
    "\n",
    "final_training_df.to_pickle(\"filepath\")\n",
    "final_test_df.to_pickle(\"filepath\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
